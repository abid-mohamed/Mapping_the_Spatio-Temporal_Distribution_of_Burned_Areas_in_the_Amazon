---
title: "Amazon_methods"
date: "`r Sys.Date()`"
output:
  html_document: 
    number_sections: yes
    fig_caption: yes
    toc: true
    toc_float: 
      collapsed: true
    theme: cerulean
    highlight: kate
    toc_depth: 5
    keep_md: yes
    df_print: paged
---

# Load libraries

```{r warning=FALSE, message=FALSE}
library(terra)
library(sf)
library(tidyverse)
library(data.table)
library(dplyr)
library(gdata)
library(pROC)
# Parallel
library(foreach)
library(doSNOW)
library(parallel)
library(itertools)
# Cross Validation
library(rsample)
library(recipes)
# Plot
library(RColorBrewer)
library(tidyterra)
library(scico)
library(wesanderson)
library(viridis)
library(scales)
# library(latex2exp)
library(ggplot2)
library(plotly)
# Time
library(tictoc)
library(gdata)
```

# Functions

```{r "Functions"}
random_dt <- function(DT) {
  DT.nrow <- nrow(DT)
  samp <- sample(1:DT.nrow, DT.nrow, replace=F)
  return(DT[samp,])
}

splitAmaZones <- function(Amazon.data.dt, amaz.basin){
  #---- Initialization of zones ----
  amaz.basin.shp <- st_read("/home/abidm/Documents/Amazon_new_data/0. Amazon_shapefile/projected/amazon_shp_projected.shp")
  v <- ext(amaz.basin.shp)
  x.min <- v$xmin[[1]] - 1; x.max <- v$xmax[[1]] + 1; 
  y.min <- v$ymin[[1]] - 1; y.max <- v$ymax[[1]] + 1; 
  #
  XY <- list(
    c(x.min, -0.84e+06, y.min, y.max),                                    # z1
    c(-0.84e+06, +0.55e+06,  3.75e+06,     y.max),                        # z2
    c(-0.84e+06, +0.55e+06,  2.60e+06,  3.75e+06),                        # z3
    c(-0.84e+06, -0.05e+06,  2.22e+06,  2.60e+06),                        # z4
    c(-0.84e+06, -0.51e+06,     y.min,  2.22e+06),                        # z5
    c(-0.51e+06, -0.05e+06,     y.min,  2.22e+06),                        # z6
    c(-0.05e+06,  0.60e+06,     y.min,  2.60e+06),                        # z7
    c( 0.60e+06,  0.95e+06,     y.min,  2.60e+06),                        # z8
    c(+0.55e+06,     x.max,  3.24e+06,     y.max),                        # z9
    c(+0.55e+06,     x.max,  2.86e+06,  3.24e+06),                        # z10
    c(+0.55e+06,  0.95e+06,     x.max,      y.min,  2.60e+06,  2.86e+06)  # z11
  )
  #---- Create column `Zones` ----
  Amazon.data.dt[, Zones := 0.0]
  Amazon.data.dt %>% setcolorder(., c('cell', 'x', 'y', 'Zones', 'Year', 'Month')) %>% as.data.table()
  nz <- length (XY)
  #---- Split the data ----
  for (i in 1:nz){
    cat("Zone", i, "/", nz, " - ")
    if (i == nz){
      # Polygon
      Amazon.data.dt[((x > XY[[i]][2] & x <= XY[[i]][3]) & (y > XY[[i]][4] & y <= XY[[i]][5])) | 
                       ((x > XY[[i]][1] & x <= XY[[i]][3]) & (y > XY[[i]][5] & y <= XY[[i]][6])), Zones := i]
    }else{
      # Rectangle
      Amazon.data.dt[(x > XY[[i]][1] & x <= XY[[i]][2]) & (y > XY[[i]][3] & y <= XY[[i]][4]), Zones := i]
    }
  }
  #---- Compute the number of cells in each zone ----
  dim.zones <- Amazon.data.dt[, .N, by=.(Zones)]
  dim.zones[order(Zones)]
  return(Amazon.data.dt)
}

#---- Plot ----
myPlot <- function(
  rast, 
  title=NULL, 
  sub_title=NULL, 
  theme=2, 
  na_value=NULL, 
  max_cell=1e6,
  x_angle=0,
  b_size=16,
  v_unit=c(2, 2, 2, 2) # c(t, r, b, l)
){
  # Prepare missing data
  if (!is.null(na_value)) {
    min.rast <- minmax(rast)[1]
    rast.na <- rast
    rast.na[is.na(rast.na)] <- min.rast - 1e3
    rast.na[rast.na >= min.rast] <- NA
    # levels(rast.na) <- data.frame(id=c(min.rast - 1e3), val=c('1'))
    rast.na <- rast.na %>% mask(mask = amaz.basin.shp)
    # rast.na[rast.na < min.rast] <- -1e-09
    p1 <- ggplot() +
      geom_spatraster(data = rast.na, maxcell = max_cell)
  }else{
    p1 <- ggplot()
  }
  p1 <- switch(
    theme,
    p1,
    p1 + theme_bw(base_size=b_size),
    p1 + theme_linedraw(base_size=b_size),
    p1 + theme_light(base_size=b_size),
    p1 + theme_minimal(base_size=b_size),
    p1 + theme_classic(base_size=b_size),
    p1 + theme_gray(base_size=b_size),
    p1 + theme_dark(base_size=b_size)
  )
  p1 <- p1  +
    geom_spatraster(data = rast, maxcell = max_cell) +
    geom_spatvector(data = amaz.basin.shp$geometry, fill = NA, color = "gray40") +
    scale_x_continuous(labels = function(x) format(x, scientific = T)) +
    scale_y_continuous(labels = function(x) format(x, scientific = T)) +
    ggtitle(label=title, subtitle=sub_title) +
    coord_sf(datum = pull_crs(rast)) +
    theme(
      axis.text.x = element_text(angle = x_angle)
      , plot.margin = unit(v_unit, "pt")
    )
  if (theme == 1) {p1 <- p1 + theme_void(base_size=b_size)}
  # coord_sf(crs = st_crs(4326))
  return(p1)
}
```

# Initialization

```{r}
# Import shape file
amaz.basin.shp <- st_read("~/Documents/Amazon_new_data/0. Amazon_shapefile/projected/amazon_shp_projected.shp")

# path of variables
my.path <- "~/Documents"
path0 <- "/home/abidm/Documents"
burntArea.path <- paste0(path0, "/Amazon_new_data/1. Burnt Area/03. Working Data")
landCover.path <- paste0(path0, "/Amazon_new_data/2. Land Cover/03. Working Data")
precip.path <- paste0(path0, "/Amazon_new_data/3. Precipitation/03. Working Data")
soilMoisture.path <- paste0(path0, "/Amazon_new_data/4. Soil Moisture/03. Working Data")
elevation.path <- paste0(path0, "/Amazon_new_data/5. Elevation/03. Working Data")
LandSurfaceTemp.path <- paste0(path0, "/Amazon_new_data/6. LandSurfaceTemp/03. Working Data")
humidity.path <- paste0(path0, "/Amazon_new_data/7. Specific Humidity/03. Working Data")
evapotranspiration.path <- paste0(path0, "/Amazon_new_data/8. Evapotranspiration/03. Working Data")
wind.path <- paste0(path0, "/Amazon_new_data/9. Wind Speed/03. Working Data")
airtemp.path <- paste0(path0, "/Amazon_new_data/10. Air Temperature/03. Working Data")

# Color palette
my.colors <- c("mediumblue", "mediumseagreen", "firebrick")
pal <- colorRampPalette(c("mediumblue", "mediumseagreen", "firebrick"))

# Create a sequence of dates
sq.date <- seq(as.Date("2001-1-1"), as.Date("2020-12-1"), by = "month") %>% 
  format(., '%Y_%m') %>% 
  setdiff(., c("2012_07", "2012_09"))
```

# Prepare data

## Load data ---- \\

```{r}
# tic() # 81.636 sec elapsed
# load(paste0(my.path,"/Amazon_selected_data/Amazon_zones_dt.Rdata"))
# toc()
# # Convert column `Zones` as factor
# Amazon.data.dt$Zones <- as.factor(Amazon.data.dt$Zones)

#--- Save data with zones
# save(Amazon.data.dt, file = paste0(my.path,"/Amazon_selected_data/Amazon_zonesFactor_dt.Rdata"))
tic() # 83.374 sec elapsed
load(paste0(my.path,"/Amazon_selected_data/Amazon_zonesFactor_dt.Rdata")) # Amazon.data.dt
toc()
Amazon.data.dt <- Amazon.data.dt[, c('cell'):=NULL]
```

## Normalize data \\

```{r}
# recipe
AZ_recipe <- recipe( BurntArea ~ ., data = Amazon.data.dt) %>% step_normalize(all_numeric(), -c(x, y))
  # step_normalize(
  #   Precipitation, SoilMoisture, Elevation, LandSurfaceTemp, Humidity, Evapotranspiration, Wind, AirTemp
  # )
# Normalize data
tic() # 281.414 sec elapsed
AZ.norm <- AZ_recipe %>% prep() %>% bake(new_data = NULL) %>% setcolorder(., c('Zones', 'x', 'y', 'Year', 'Month', 'BurntArea'))
toc()
rm(AZ_recipe)
gc()

#--- Save data
# save(AZ.norm, file = paste0(my.path,"/Amazon_selected_data/AZ_norm.Rdata"))
tic() # 77.443 sec elapsed | 273.79
load(paste0(path0,"/Amazon_selected_data/AZ_norm.Rdata"))
toc()
```

## Split data to training and testing data by zone \\

```{r}
tic() # 762.731 sec elapsed ~ 12.7 min
nz <- AZ.norm$Zones %>% levels() %>% length()
for  (i in 1:nz){
  cat(" - Zone", i)
  AZ.nz <- AZ.norm[AZ.norm$Zones == i,]
  # Split data
  split <- initial_split(AZ.nz, prop = 0.75, strata = BurntArea)
  AZ.trn <- training(split)
  AZ.tst <- testing(split)
  # Save data
  save(AZ.nz, AZ.trn, AZ.tst, file = paste0(my.path,"/Amazon_selected_data/data_by_zones/AZ", i, "_data_train_test.Rdata"))
  rm(AZ.nz, AZ.trn, AZ.tst)
}
toc()
```

## Create v-fold cross validation samples 

```{r}
set.seed(1)
nbr.folds <- 2
for (zone in 1:11) {
  tic()
  cat("\n - Zone", zone)
  load(paste0(my.path,"/Amazon_selected_data/data_by_zones/AZ", zone, "_data_train_test.Rdata"))
  AZ.cv <- vfold_cv(AZ.trn, v = nbr.folds,  strata = BurntArea)
  
  AZ.folds.lst <- list()
  for (f in 1:nbr.folds){
    cat(" - Fold", f)
    AZ.folds.lst[[f]] <- AZ.cv$splits[[f]] %>% assessment() %>% add_column(Folds = f, .before = "BurntArea")
    AZ.folds.lst[[f]]$Folds <- as.factor(AZ.folds.lst[[f]]$Folds)
  }
  AZ.trn.folds <- rbindlist(AZ.folds.lst, use.names = T, fill = T)
  save(AZ.nz, AZ.trn, AZ.tst, AZ.trn.folds, 
       file = paste0(my.path,"/Amazon_selected_data/data_by_zones_folds/AZ", zone, "_data_train_test_", nbr.folds, "f.Rdata"))
  toc()
  rm(AZ.nz, AZ.trn, AZ.tst, AZ.trn.folds)
}
```

# Initialize and Connect to H2O 

```{r}
options(java.parameters = "-Xmx400g")
Sys.setenv("OPENBLAS_MAIN_FREE"=1)

library(h2o)
h2o.init(ip = 'localhost', port = 50001, nthreads = 20, max_mem_size = '400g')
```

```{r}
h2o.removeAll()
h2o:::.h2o.garbageCollect()
h2o:::.h2o.garbageCollect()
h2o:::.h2o.garbageCollect()
h2o.ls()
# h2o.shutdown(prompt = FALSE)
```

# Methods

## Gradient Boosting Machine (GBM)

```{r}
NumberOfCluster <- 7
cl <- makeCluster(NumberOfCluster) 
registerDoSNOW(cl)

# init the progress bar
pb <- txtProgressBar(max = 100, style = 3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress = progress)

options(java.parameters = "-Xmx700g")
Sys.setenv("OPENBLAS_MAIN_FREE"=1)

tic() # 2082.204 sec elapsed
#---- conduct the parallelism ----
Result.list <- foreach(
  zone = 1:11,
  .combine='rbind',
  .packages=c('dplyr', 'tidyverse', 'h2o'),
  .options.snow = opts) %dopar% {
    zone <- zone + 1
    my.port <- 50001 + 3 * zone
    # print(paste0("http://localhost:", my.port))
    h2o.init(nthreads = 7, max_mem_size = "90G", ip = 'localhost', port = my.port)
    h2o.removeAll()
    h2o:::.h2o.garbageCollect()
    h2o:::.h2o.garbageCollect()
    h2o:::.h2o.garbageCollect()
    h2o.ls()
    h2o.clusterInfo()
    h2o.clusterStatus()

    #---- Load data ----
    load(paste0(my.path,"/Amazon_selected_data/data_by_zones_folds/AZ", zone, "_data_train_test_3f.Rdata"))
    # convert to H2O object
    AZ.h2o <- as.h2o(AZ.nz)
    train.h2o <- as.h2o(AZ.trn.folds)
    test.h2o <- as.h2o(AZ.tst)
    # 
    var.names <- colnames(AZ.trn.folds)
    Y <- "BurntArea"
    covt.names <- setdiff(var.names, c("BurntArea", "Zones", "Folds"))
  
    #---- fit the model ----
    n.trees = 1000; m.tries = 0.6
    gbm.az <- h2o.gbm(
      model_id = paste0("GBM_AZ", zone),
      x = covt.names, 
      y = Y, 
      training_frame = train.h2o,
      # seed = 100, 
      # nfolds = 10,
      # fold_assignment = 'Stratified',
      fold_column = "Folds",
      keep_cross_validation_predictions = TRUE,
      ntrees = n.trees, 
      col_sample_rate = m.tries,
      score_each_iteration = TRUE
    )
    #---- Save the model ----
    gbm.az.path <- h2o.saveModel(
      object = gbm.az,
      path = paste0(path0, "/Models_cv/GBM"),
      force = FALSE,
      export_cross_validation_predictions = TRUE,
      filename = paste0("GBM_AZ", zone, "_model")
    )
    
    #---- Load the model ----
    # gbm.az <- h2o.upload_model(paste0(path0, "/Models/GBM/GBM_AZ", zone, "_model"))
    
    #---- Performance ----
    # 88.472 sec elapsed
    gbm.az.perf <- h2o.performance(gbm.az, test.h2o)
    metrics <- c('MSE', 'RMSE', 'LogLoss', 'Mean Per-Class Error', 'AUC', 'AUCPR', 'Gini', 'R^2')
    values <- c(
      gbm.az.perf@metrics$MSE,
      gbm.az.perf@metrics$RMSE,
      gbm.az.perf@metrics$logloss,
      gbm.az.perf@metrics$mean_per_class_error,
      gbm.az.perf@metrics$AUC,
      gbm.az.perf@metrics$pr_auc,
      gbm.az.perf@metrics$Gini,
      gbm.az.perf@metrics$r2
    )
    perf.df <- data.frame(metrics, values)
    confusion.mtrx <- gbm.az.perf@metrics$cm$table
    max.metrics <- gbm.az.perf@metrics$max_criteria_and_metric_scores
    save(perf.df, confusion.mtrx, max.metrics, file = paste0(path0, "/Models_cv/GBM/GBM_AZ", zone, "_perf.Rdata"))
    # load(paste0(path0, "/Models_cv/GBM/GBM_AZ", zone, "_perf.Rdata"))
    
    #---- Prediction ---- 
    # 155.708 sec elapsed
    tmp <- h2o.predict(gbm.az, newdata=AZ.h2o)
    gbm.az.predic <- as.data.frame(tmp)
    # Save
    save(gbm.az.predic, file = paste0(path0, "/Models_cv/GBM/GBM_AZ", zone, "_predic.Rdata"))
    # load(paste0(path0, "/Models_cv/GBM/GBM_AZ", zone, "_predic.Rdata"))
  }
# close progress bar
close(pb)
# stop cluster
stopCluster(cl) 
toc()

#----

nbr.folds <- 2
for (zone in 2:11){
  tic()
  # zone <- 1
  cat('- zone', zone)
  #---- Load data ----
  load(paste0(my.path,"/Amazon_selected_data/data_by_zones_folds/AZ", zone, "_data_train_test_", nbr.folds, "f.Rdata"))
  # convert to H2O object
  AZ.h2o <- as.h2o(AZ.nz)
  train.h2o <- as.h2o(AZ.trn.folds)
  test.h2o <- as.h2o(AZ.tst)
  # 
  var.names <- colnames(AZ.trn.folds)
  Y <- "BurntArea"
  covt.names <- setdiff(var.names, c("BurntArea", "Zones", "Folds"))
  
  #---- fit the model ----
  # 1314.247 sec elapsed ~ 22 min
  tic()
  n.trees = 1000; m.tries = 0.6
  gbm.az <- h2o.gbm(
    model_id = paste0("GBM_AZ", zone),
    x = covt.names, 
    y = Y, 
    training_frame = train.h2o,
    # seed = 100, 
    # nfolds = 10,
    # fold_assignment = 'Stratified',
    fold_column = "Folds",
    keep_cross_validation_predictions = TRUE,
    ntrees = n.trees, 
    col_sample_rate = m.tries,
    score_each_iteration = TRUE
  )
  toc()
  
  #---- Save the model ----
  gbm.az.path <- h2o.saveModel(
    object = gbm.az,
    path = paste0(path0, "/Models_cv/GBM"),
    force = FALSE,
    export_cross_validation_predictions = TRUE,
    filename = paste0("GBM_AZ", zone, "_model_", nbr.folds, "f")
  )
  
  #---- Load the model ----
  # gbm.az <- h2o.upload_model(paste0(path0, "/Models_cv/GBM/GBM_AZ", zone, "_model_", nbr.folds, "f"))
  
  #---- Performance ----
  # 88.472 sec elapsed
  tic()
  gbm.az.perf <- h2o.performance(gbm.az, test.h2o)
  metrics <- c('MSE', 'RMSE', 'LogLoss', 'Mean Per-Class Error', 'AUC', 'AUCPR', 'Gini', 'R^2')
  values <- c(
    gbm.az.perf@metrics$MSE,
    gbm.az.perf@metrics$RMSE,
    gbm.az.perf@metrics$logloss,
    gbm.az.perf@metrics$mean_per_class_error,
    gbm.az.perf@metrics$AUC,
    gbm.az.perf@metrics$pr_auc,
    gbm.az.perf@metrics$Gini,
    gbm.az.perf@metrics$r2
  )
  perf.df <- data.frame(metrics, values)
  confusion.mtrx <- gbm.az.perf@metrics$cm$table
  max.metrics <- gbm.az.perf@metrics$max_criteria_and_metric_scores
  save(perf.df, confusion.mtrx, max.metrics, file = paste0(path0, "/Models_cv/GBM/GBM_AZ", zone, "_perf_", nbr.folds, "f.Rdata"))
  # load(paste0(path0, "/Models_cv/GBM/GBM_AZ", zone, "_perf_", nbr.folds, "f.Rdata"))
  toc()
  
  #---- Variable importance ----
  # Retrieve variable importance
  # h2o.varimp(gbm.az)
  
  #---- Plot variable importance ----
  # h2o.varimp_plot(model = gbm.az, num_of_features = 13)
  
  #---- Prediction ---- 
  # 155.708 sec elapsed
  # tmp.test <- h2o.predict(gbm.az, newdata=test.h2o)
  # gbm.az.predic.test <- as.data.frame(tmp.test)

  # roc(as.numeric(AZ.tst$BurntArea), as.numeric(gbm.az.predic.test$predict), plot=T, auc=T)  
 
  tic()
  tmp <- h2o.predict(gbm.az, newdata=AZ.h2o)
  gbm.az.predic <- as.data.frame(tmp)
  # Save
  save(gbm.az.predic, file = paste0(path0, "/Models_cv/GBM/GBM_AZ", zone, "_predic_", nbr.folds, "f.Rdata"))
  # load(paste0(path0, "/Models_cv/GBM/GBM_AZ", zone, "_predic_", nbr.folds, "f.Rdata"))
  
  toc() # 2014.269 sec elapsed ~ 34 min
  
  # Remove variables
  rm(gbm.az, gbm.az.perf, perf.df, confusion.mtrx, max.metrics, gbm.az.predic)
  rm(AZ.nz, AZ.trn, AZ.tst); gc(); gc()
  h2o.removeAll()
  h2o:::.h2o.garbageCollect(); Sys.sleep(20)
  h2o:::.h2o.garbageCollect(); Sys.sleep(20)
}
```

```{r}
zone <- zone + 1
    my.port <- 50001
    # print(paste0("http://localhost:", my.port))
    h2o.init(nthreads = 7, max_mem_size = "90G", ip = 'localhost', port = my.port)
    h2o.ls()
    h2o.removeAll()
    h2o:::.h2o.garbageCollect()
    h2o:::.h2o.garbageCollect()
    h2o:::.h2o.garbageCollect()
    h2o.ls()
    h2o.clusterInfo()
    h2o.clusterStatus()
    h2o.shutdown(prompt = FALSE)
```


## Random Forest (RF)

```{r}
nbr.folds <- 2
for (zone in 3:11){
  # zone <- 1
  tic()
  cat('- zone', zone)
  #---- Load data ----
  load(paste0(my.path,"/Amazon_selected_data/data_by_zones_folds/AZ", zone, "_data_train_test_", nbr.folds, "f.Rdata"))
  # convert to H2O object
  AZ.h2o <- as.h2o(AZ.nz)
  train.h2o <- as.h2o(AZ.trn.folds)
  test.h2o <- as.h2o(AZ.tst)
  # 
  var.names <- colnames(AZ.trn)
  Y <- "BurntArea"
  covt.names <- setdiff(var.names, c("BurntArea", "Zones", "Folds"))
  
  #---- fit the model ----
  tic()
  # __ sec elapsed ~ 1.7 h
  n.trees = 500; m.tries = 7
  rf.az <- h2o.randomForest(
    model_id = paste0("RF_AZ", zone),
    x = covt.names, 
    y = Y, 
    training_frame = train.h2o,
    fold_column = "Folds",
    keep_cross_validation_predictions = TRUE,
    ntrees = n.trees, 
    mtries = m.tries,
    score_each_iteration = TRUE
  )
  toc()

  #---- Save the model ----
  rf.az.path <- h2o.saveModel(
    object = rf.az,
    path = paste0(path0, "/Models_cv/RF"),
    force = FALSE,
    export_cross_validation_predictions = TRUE,
    filename = paste0("RF_AZ", zone, "_model_", nbr.folds, "f")
  )

  #---- Load the model ----
  # rf.az <- h2o.upload_model(paste0(path0, "/Models/RF/RF_AZ", zone, "_model_", nbr.folds, "f"))
  
  #---- Performance ----
  # 364.984 sec elapsed ~ 6 min
  rf.az.perf <- h2o.performance(rf.az, test.h2o)
  metrics <- c('MSE', 'RMSE', 'LogLoss', 'Mean Per-Class Error', 'AUC', 'AUCPR', 'Gini', 'R^2')
  values <- c(
    rf.az.perf@metrics$MSE,
    rf.az.perf@metrics$RMSE,
    rf.az.perf@metrics$logloss,
    rf.az.perf@metrics$mean_per_class_error,
    rf.az.perf@metrics$AUC,
    rf.az.perf@metrics$pr_auc,
    rf.az.perf@metrics$Gini,
    rf.az.perf@metrics$r2
  )
  perf.df <- data.frame(metrics, values)
  confusion.mtrx <- rf.az.perf@metrics$cm$table
  max.metrics <- rf.az.perf@metrics$max_criteria_and_metric_scores
  save(perf.df, confusion.mtrx, max.metrics, file = paste0(path0, "/Models_cv/RF/RF_AZ", zone, "_perf_", nbr.folds, "f.Rdata"))
  # load(paste0(path0, "/Models_cv/RF/RF_AZ", zone, "_perf_", nbr.folds, "f.Rdata"))

  #---- Variable importance ----
  # Retrieve variable importance
  # h2o.varimp(rf.az)
  
  #---- Plot variable importance ----
  # h2o.varimp_plot(model = rf.az, num_of_features = 13)
  
  #---- Prediction ---- 
  # 1619.241 sec elapsed ~ 27 min
  tmp <- h2o.predict(rf.az, newdata=AZ.h2o)
  rf.az.predic <- as.data.frame(tmp)
  # Save
  save(rf.az.predic, file = paste0(path0, "/Models_cv/RF/RF_AZ", zone, "_predic_", nbr.folds, "f.Rdata"))
  # load(paste0(path0, "/Models_cv/RF/RF_AZ", zone, "_predic_", nbr.folds, "f.Rdata"))
  
  # Remove variables
  rm(AZ.nz, AZ.trn, AZ.tst); gc(); gc()
  h2o.removeAll()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  toc() # 9144.623 sec elapsed ~ 2.5 h
}
```

## XGBoost 

```{r}
for (zone in 1:11){
  # zone <- 2
  tic()
  cat('- zone', zone)
  #---- Load data ----
  load(paste0(my.path,"/Amazon_selected_data/data_by_zones/AZ", zone, "_data_train_test.Rdata"))
  # convert to H2O object
  AZ.h2o <- as.h2o(AZ.nz)
  train.h2o <- as.h2o(AZ.trn)
  test.h2o <- as.h2o(AZ.tst)
  # 
  var.names <- colnames(AZ.trn)
  Y <- "BurntArea"
  covt.names <- setdiff(var.names, c("BurntArea", "Zones"))
  
  #---- fit the model ----
  # 38797.326 sec elapsed ~ 10.7 h
  n.trees = 1000; m.tries = 0.6
  xgb.az <- h2o.xgboost(
    model_id = paste0("XGB_AZ", zone),
    x = covt.names, 
    y = Y, 
    training_frame = train.h2o,
    # fold_column = "Folds",
    # keep_cross_validation_predictions = TRUE,
    ntrees = n.trees, 
    col_sample_rate = m.tries,
    score_each_iteration = TRUE
  )

  #---- Save the model ----
  xgb.az.path <- h2o.saveModel(
    object = xgb.az,
    path = paste0(path0, "/Models/XGB"),
    force = FALSE,
    export_cross_validation_predictions = TRUE,
    filename = paste0("XGB_AZ", zone, "_model")
  )

  #---- Load the model ----
  # xgb.az <- h2o.upload_model(paste0(path0, "/Models/XGB/XGB_AZ", zone, "_model"))
  
  #---- Performance ----
  # 30.075 sec elapsed
  xgb.az.perf <- h2o.performance(xgb.az, test.h2o)
  metrics <- c('MSE', 'RMSE', 'LogLoss', 'Mean Per-Class Error', 'AUC', 'AUCPR', 'Gini', 'R^2')
  values <- c(
    xgb.az.perf@metrics$MSE,
    xgb.az.perf@metrics$RMSE,
    xgb.az.perf@metrics$logloss,
    xgb.az.perf@metrics$mean_per_class_error,
    xgb.az.perf@metrics$AUC,
    xgb.az.perf@metrics$pr_auc,
    xgb.az.perf@metrics$Gini,
    xgb.az.perf@metrics$r2
  )
  perf.df <- data.frame(metrics, values)
  confusion.mtrx <- xgb.az.perf@metrics$cm$table
  max.metrics <- xgb.az.perf@metrics$max_criteria_and_metric_scores
  save(perf.df, confusion.mtrx, max.metrics, file = paste0(path0, "/Models/XGB/XGB_AZ", zone, "_perf.Rdata"))

  #---- Variable importance ----
  # Retrieve variable importance
  # h2o.varimp(xgb.az)
  
  #---- Plot variable importance ----
  # h2o.varimp_plot(model = xgb.az, num_of_features = 13)
  
  #---- Prediction ---- 
  # 358.52 sec elapsed ~ __ min
  tmp <- h2o.predict(xgb.az, newdata=AZ.h2o)
  xgb.az.predic <- as.data.frame(tmp)
  # Save
  save(xgb.az.predic, file = paste0(path0, "/Models/XGB/XGB_AZ", zone, "_predic.Rdata"))
  # load(paste0(path0, "/Models/XGB/XGB_AZ", zone, "_predic.Rdata"))

  # Remove variables
  rm(AZ.nz, AZ.trn, AZ.tst); gc(); gc()
  h2o.removeAll()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  toc() # 38635.03 sec elapsed ~ 10.7 h
}
```

## GLM 

```{r}
nbr.folds <- 1
for (zone in 4:11){
  tic()
  # zone <- 1
  cat('- zone', zone)
  #---- Load data ----
  # load(paste0(my.path,"/Amazon_selected_data/data_by_zones/AZ", zone, "_data_train_test.Rdata"))
  load(paste0(my.path,"/Amazon_selected_data/data_by_zones_folds/AZ", zone, "_data_train_test_", nbr.folds, "f.Rdata"))
  # convert to H2O object
  AZ.h2o <- as.h2o(AZ.nz)
  # train.h2o <- as.h2o(AZ.trn)
  train.h2o <- as.h2o(AZ.trn.folds)
  test.h2o <- as.h2o(AZ.tst)
  # 
  var.names <- colnames(AZ.trn)
  Y <- "BurntArea"
  covt.names <- setdiff(var.names, c("BurntArea", "Zones", "Folds"))
  
  #---- fit the model ----
  # 19.372 sec elapsed
  tic()
  glm.az <- h2o.glm(
    model_id = paste0("GLM_AZ", zone),
    x = covt.names, 
    y = Y, 
    training_frame = train.h2o,
    family="binomial",
    # alpha=0,
    # seed = 1,
    # nfolds = 1,
    # fold_assignment = 'Stratified',
    fold_column = "Folds",
    keep_cross_validation_predictions = TRUE,
    standardize=F,
    lambda = 0,
    # lambda_search=T,
    # nlambdas=200,
    # lambda_min_ratio=0.00001,
    score_each_iteration = TRUE
  )
  toc()
  
  #---- Save the model ----
  glm.az.path <- h2o.saveModel(
    object = glm.az,
    path = paste0(path0, "/Models_cv/GLM"),
    force = FALSE,
    export_cross_validation_predictions = TRUE,
    filename = paste0("GLM_AZ", zone, "_model_", nbr.folds, "f")
  )

  #---- Load the model ----
  # glm.az <- h2o.upload_model(paste0(path0, "/Models_cv/GLM/GLM_AZ", zone, "_model_", nbr.folds, "f"))

  #---- Performance ----
  # 1.086 sec elapsed
  glm.az.perf <- h2o.performance(glm.az, test.h2o)
  metrics <- c('MSE', 'RMSE', 'LogLoss', 'Mean Per-Class Error', 'AUC', 'AUCPR', 'Gini', 'R^2')
  values <- c(
    glm.az.perf@metrics$MSE,
    glm.az.perf@metrics$RMSE,
    glm.az.perf@metrics$logloss,
    glm.az.perf@metrics$mean_per_class_error,
    glm.az.perf@metrics$AUC,
    glm.az.perf@metrics$pr_auc,
    glm.az.perf@metrics$Gini,
    glm.az.perf@metrics$r2
  )
  perf.df <- data.frame(metrics, values)
  confusion.mtrx <- glm.az.perf@metrics$cm$table
  max.metrics <- glm.az.perf@metrics$max_criteria_and_metric_scores
  save(perf.df, confusion.mtrx, max.metrics, file = paste0(path0, "/Models_cv/GLM/GLM_AZ", zone, "_perf_", nbr.folds, "f.Rdata"))
  # load(paste0(path0, "/Models_cv/GLM/GLM_AZ", zone, "_perf.Rdata"))

  #---- Variable importance ----
  # Retrieve variable importance
  # h2o.varimp(glm.az)
  
  #---- Plot variable importance ----
  # h2o.varimp_plot(model = glm.az, num_of_features = 13)
  
  #---- Prediction ---- 
  # __ sec elapsed ~ __ min
  tmp <- h2o.predict(glm.az, newdata=AZ.h2o)
  # options("datatable.verbose"=TRUE)
  # options("h2o.use.data.table"=TRUE)  # use data.table
  glm.az.predic <- h2o:::as.data.frame.H2OFrame(tmp)
  # Save
  save(glm.az.predic, file = paste0(path0, "/Models_cv/GLM/GLM_AZ", zone, "_predic_", nbr.folds, "f.Rdata"))
  # load(paste0(path0, "/Models_cv/GLM/GLM_AZ", zone, "_predic.Rdata"))
  
  toc() # 439.142 sec elapsed ~ 8 min

  # Remove variables
  rm(glm.az, glm.az.perf, perf.df, confusion.mtrx, max.metrics, glm.az.predic)
  rm(AZ.nz, AZ.trn, AZ.tst); gc(); gc()
  h2o.removeAll()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
}
```

```{r}
for (zone in 1:11){
  tic()
  # zone <- 1
  cat('- zone', zone)
  #---- Load data ----
  load(paste0(my.path,"/Amazon_selected_data/data_by_zones/AZ", zone, "_data_train_test.Rdata"))
  var.names <- colnames(AZ.trn)
  Y <- "BurntArea"
  covt.names <- setdiff(var.names, c("BurntArea", "Zones"))
  
  #---- fit the model ----
  tic()
  # __ sec elapsed ~ __ min
  glm.az <- glm(BurntArea ~ ., family = 'binomial', data = AZ.trn[, -1])
  summary(glm.az)
  toc()

  #---- Save the model ----
  save(glm.az, file = paste0(path0, "/Models/GLMr/GLMr_AZ", zone, "_model.rda"))
  #---- Load the model ----
  load(paste0(path0, "/Models/GLMr/GLMr_AZ", zone, "_model.rda"))
  
  #---- Performance ----
  # __ sec elapsed ~ __ min
  tmp.predc <- predict(f, AZ.tst, type="response")
  glm.az.roc <- roc(AZ.tst$BurntArea, tmp.predc)

  #---- Prediction ----
  # __ sec elapsed ~ __ min
  glm.az.predic <- predict(glm.az, newdata=AZ.nz, type="response")
  glm.az.predic <- as.data.frame(tmp)
  # Save
  save(glm.az.predic, file = paste0(path0, "/Models/GLMr/GLMr_AZ", zone, "_predic.Rdata"))
  # load(paste0(path0, "/Models/GLMr/GLMr_AZ", zone, "_predic.Rdata"))

  
  
  f <- glm(BurntArea ~ ., family = 'binomial', data = AZ.trn[,-1])  
  #---- Save / Load ----
  save(m1, file = "my_model1.rda")
  
  load(paste0(path0, "/Models/GBM/GBM_AZ", zone, "_predic.Rdata"))
  
  gbm.az.predic
  
  predicted <- predict(f, AZ.tst, type="response")
  predicted_rocr <- prediction(gbm.az.predic$p1, AZ.nz$BurntArea)
  roc_object <- roc(AZ.tst$BurntArea, predicted, plot=T)
  roc_object$auc
  
  library(ROCR)
  f.tmp <- ROCR::performance(predicted_rocr,"f")
  plot(f.tmp)
  auc.tmp@y.values
}
```

```{r}
# library(easystats)
```

## SVM 

```{r}
for (zone in 1:11){
  # zone <- 1
  tic()
  cat('- zone', zone)
  #---- Load data ----
  load(paste0(my.path,"/Amazon_selected_data/data_by_zones/AZ", zone, "_data_train_test.Rdata"))
  # convert to H2O object
  AZ.h2o <- as.h2o(AZ.nz)
  train.h2o <- as.h2o(AZ.trn)
  test.h2o <- as.h2o(AZ.tst)
  # 
  var.names <- colnames(AZ.trn)
  Y <- "BurntArea"
  covt.names <- setdiff(var.names, c("BurntArea", "Zones"))
  
  #---- fit the model ----
  # 379.639 sec elapsed ~ 7 min
  tic()
  svm.az <- h2o.psvm(
    model_id = paste0("SVM_AZ", zone),
    x = covt.names, 
    y = Y, 
    training_frame = train.h2o,
    gamma = 5
    # disable_training_metrics = TRUE,
    # kernel_type = 'gaussian'
  )
  toc()

  #---- Save the model ----
  svm.az.path <- h2o.saveModel(
    object = svm.az,
    path = paste0(path0, "/Models/SVM"),
    force = FALSE,
    export_cross_validation_predictions = TRUE,
    filename = paste0("SVM_AZ", zone, "_model")
  )

  #---- Load the model ----
  # h2o.upload_model(paste0(path0, "/Models/SVM/SVM_AZ", zone, "_model"))
  
  #---- Performance ----
  # 30.075 sec elapsed
  tic()
  svm.az.perf <- h2o.performance(svm.az, test.h2o)
  toc()
  metrics <- c('MSE', 'RMSE', 'LogLoss', 'Mean Per-Class Error', 'AUC', 'AUCPR', 'Gini', 'R^2')
  values <- c(
    svm.az.perf@metrics$MSE,
    svm.az.perf@metrics$RMSE,
    svm.az.perf@metrics$logloss,
    svm.az.perf@metrics$mean_per_class_error,
    svm.az.perf@metrics$AUC,
    svm.az.perf@metrics$pr_auc,
    svm.az.perf@metrics$Gini,
    svm.az.perf@metrics$r2
  )
  perf.df <- data.frame(metrics, values)
  confusion.mtrx <- svm.az.perf@metrics$cm$table
  max.metrics <- svm.az.perf@metrics$max_criteria_and_metric_scores
  save(perf.df, confusion.mtrx, max.metrics, file = paste0(path0, "/Models/SVM/SVM_AZ", zone, "_perf.Rdata"))
toc()

  #---- Variable importance ----
  # Retrieve variable importance
  # h2o.varimp(svm.az)
  
  #---- Plot variable importance ----
  # h2o.varimp_plot(model = svm.az, num_of_features = 13)
  
  #---- Prediction ---- 
  # 358.52 sec elapsed ~ __ min
  tmp <- h2o.predict(svm.az, newdata=AZ.h2o)
  svm.az.predic <- as.data.frame(tmp)
  # Save
  save(svm.az.predic, file = paste0(path0, "/Models/SVM/SVM_AZ", zone, "_predic.Rdata"))
  # load(paste0(path0, "/Models/SVM/SVM_AZ", zone, "_predic.Rdata"))

  # Remove variables
  rm(AZ.nz, AZ.trn, AZ.tst, svm.az); gc(); gc()
  h2o.removeAll()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  toc() # 38635.03 sec elapsed ~ 10.7 h
}
```

# Ensemble E1

```{r}
library(ROCR)
tic("Total")
Models <- c('GLM', 'GBM', 'XGB', 'RF')
nbr.m <- length(Models)
nbr.z <- 11
beta.meanErr <- matrix(0, nbr.z, nbr.m)
colnames(beta.meanErr) <- Models;  rownames(beta.meanErr) <- paste0('zone', 1:nbr.z)
beta.auc <- matrix(0, nbr.z, nbr.m)
colnames(beta.auc) <- Models;  rownames(beta.auc) <- paste0('zone', 1:nbr.z)
beta.auc2 <- beta.auc
auc.m <-  matrix(0, nbr.z, 1)
colnames(auc.m) <- 'Area under the ROC curve';  rownames(auc.m) <- paste0('zone', 1:nbr.z)
p1.lst <- vector(mode='list', length=nbr.z); names(p1.lst) <- paste0('zone', 1:nbr.z)
f1.perf.lst <- vector(mode='list', length=nbr.z); names(f1.perf.lst) <- paste0('zone', 1:nbr.z)
f1.max.m <- matrix(0, nbr.z, 2)
colnames(f1.max.m) <- c('F1_max', 'cutoff');  rownames(f1.max.m) <- paste0('zone', 1:nbr.z)
pred.lst <- vector(mode='list', length=nbr.z); names(pred.lst) <- paste0('zone', 1:nbr.z)

for (zone in 1:nbr.z){
  # zone <- 1
  tic(paste0("Zone", zone))
  zone.name <- paste0('zone', zone)
  load(paste0(my.path,"/Amazon_selected_data/data_by_zones/AZ", zone, "_data_train_test.Rdata"))
  nbr.row <- dim(AZ.nz)[1]
  prob.df = data.frame(matrix(0, nrow = nbr.row, ncol = nbr.m)) 
  colnames(prob.df) = Models
  for (m in Models){
    # m <- 'GLM' 
    # Load performance measures and predictions of the model
    load(paste0(path0, "/Models/", m, "/", m, "_AZ", zone, "_perf.Rdata"))
    load(paste0(path0, "/Models/", m, "/", m, "_AZ", zone, "_predic.Rdata"))
    # Compute the parameters
    beta.meanErr[zone.name, m] <- 1 / perf.df[perf.df$metrics=='Mean Per-Class Error',]$values
    beta.auc[zone.name, m] <- 1 / (1 - perf.df[perf.df$metrics=='AUC',]$values)
    beta.auc2[zone.name, m] <- perf.df[perf.df$metrics=='AUC',]$values
    
    # Extract the probability
    mv(from = paste0(tolower(m), ".az.predic"), to = "az.predic")
    prob.df[, paste0(m)] <- az.predic$p1
    # Remove variables
    rm(perf.df, confusion.mtrx, max.metrics, az.predic)
  }
  # Normalization of the parameters
  beta.meanErr <- beta.meanErr/rowSums(beta.meanErr)
  beta.auc <- beta.auc/rowSums(beta.auc)
  beta.auc2 <- beta.auc2/rowSums(beta.auc2)
  # Compute the probability P(X=1)
  p1.lst[[zone.name]] <- Map(`*`, prob.df, beta.meanErr[zone.name, ]) %>% as.data.frame() %>% rowSums()
  # p1.lst[[zone.name]] <- p1.nz
  # Compute the F1 performance score for each threshold
  predicted.rocr <- ROCR::prediction(p1.lst[[zone.name]], AZ.nz$BurntArea)
  f1.perf <- ROCR::performance(predicted.rocr, "f")
  f1.perf.df <- data.frame(f1.perf@x.values, f1.perf@y.values); colnames(f1.perf.df) <- c(f1.perf@x.name, f1.perf@y.name)
  f1.perf.lst[[zone.name]] <- f1.perf.df
  auc.m[zone.name, 1] <- ROCR::performance(predicted.rocr, "auc")@y.values[[1]]
  # Compute the best threshold 
  ind = which.max(slot(f1.perf, "y.values")[[1]])
  f1.max = slot(f1.perf, "y.values")[[1]][ind]
  cutoff = slot(f1.perf, "x.values")[[1]][ind]
  f1.max.m[zone.name, ] <- c(f1.max, cutoff)
  # plot(f1.perf)
  # Compute the prediction
  pred.lst[[zone.name]] <- ifelse(p1.lst[[zone.name]] > cutoff, 1, 0)
  toc() # 66.9 sec elapsed
}
toc() # Total: 657.542 sec elapsed ~ 11 min
save(beta.meanErr, beta.auc, beta.auc2, file = paste0(path0, "/Models_ens/Ens1_err/ENS1_beta.Rdata"))
# load(paste0(path0, "/Models_ens/Ens1_err/ENS1_beta.Rdata"))
```

## Model ensemble 1

```{r}
#---- Install & load packages ----
# Models are built with h2o 3.38.0.1
# To install this version from cran :
# require(devtools)
# install_version("h2o", version = "3.38.0.1", repos = "http://cran.us.r-project.org", INSTALL_opts = '--no-lock')
#---

#---- Libraries and paths ----
library(data.table)
library(dplyr)
library(sf)
library(terra)
library(recipes)
library(h2o)
library(tictoc)

#---- Initialize H2O ----
options(java.parameters = "-Xmx650g")
Sys.setenv("OPENBLAS_MAIN_FREE"=1)
h2o.init(ip = 'localhost', port = 50001, nthreads = 50, max_mem_size = '650g')
h2o.no_progress()

#---- Function ----
splitAmaZones <- function(Amazon.data.dt, amaz.basin){
  #---- Initialization of zones ----
  amaz.basin.shp <- st_read("/home/abidm/Documents/Amazon_new_data/0. Amazon_shapefile/projected/amazon_shp_projected.shp")
  v <- ext(amaz.basin.shp)
  x.min <- v$xmin[[1]] - 1; x.max <- v$xmax[[1]] + 1; 
  y.min <- v$ymin[[1]] - 1; y.max <- v$ymax[[1]] + 1; 
  #
  XY <- list(
    c(x.min, -0.84e+06, y.min, y.max),                                    # z1
    c(-0.84e+06, +0.55e+06,  3.75e+06,     y.max),                        # z2
    c(-0.84e+06, +0.55e+06,  2.60e+06,  3.75e+06),                        # z3
    c(-0.84e+06, -0.05e+06,  2.22e+06,  2.60e+06),                        # z4
    c(-0.84e+06, -0.51e+06,     y.min,  2.22e+06),                        # z5
    c(-0.51e+06, -0.05e+06,     y.min,  2.22e+06),                        # z6
    c(-0.05e+06,  0.60e+06,     y.min,  2.60e+06),                        # z7
    c( 0.60e+06,  0.95e+06,     y.min,  2.60e+06),                        # z8
    c(+0.55e+06,     x.max,  3.24e+06,     y.max),                        # z9
    c(+0.55e+06,     x.max,  2.86e+06,  3.24e+06),                        # z10
    c(+0.55e+06,  0.95e+06,     x.max,      y.min,  2.60e+06,  2.86e+06)  # z11
  )
  #---- Create column `Zones` ----
  Amazon.data.dt[, Zones := 0.0]
  Amazon.data.dt %>% setcolorder(., c('cell', 'x', 'y', 'Zones', 'Year', 'Month')) %>% as.data.table()
  nz <- length (XY)
  #---- Split the data ----
  for (i in 1:nz){
    # cat("Zone", i, "/", nz, " - ")
    if (i == nz){
      # Polygon
      Amazon.data.dt[((x > XY[[i]][2] & x <= XY[[i]][3]) & (y > XY[[i]][4] & y <= XY[[i]][5])) | 
                       ((x > XY[[i]][1] & x <= XY[[i]][3]) & (y > XY[[i]][5] & y <= XY[[i]][6])), Zones := i]
    }else{
      # Rectangle
      Amazon.data.dt[(x > XY[[i]][1] & x <= XY[[i]][2]) & (y > XY[[i]][3] & y <= XY[[i]][4]), Zones := i]
    }
  }
  #---- Compute the number of cells in each zone ----
  dim.zones <- Amazon.data.dt[, .N, by=.(Zones)]
  dim.zones[order(Zones)]
  return(Amazon.data.dt)
}

#
tic("Total")
path0 <- "/home/abidm/Documents"
burntArea.path <- paste0(path0, "/Amazon_new_data/1. Burnt Area/03. Working Data")
landCover.path <- paste0(path0, "/Amazon_new_data/2. Land Cover/03. Working Data")
precip.path <- paste0(path0, "/Amazon_new_data/3. Precipitation/03. Working Data")
soilMoisture.path <- paste0(path0, "/Amazon_new_data/4. Soil Moisture/03. Working Data")
elevation.path <- paste0(path0, "/Amazon_new_data/5. Elevation/03. Working Data")
LandSurfaceTemp.path <- paste0(path0, "/Amazon_new_data/6. LandSurfaceTemp/03. Working Data")
humidity.path <- paste0(path0, "/Amazon_new_data/7. Specific Humidity/03. Working Data")
evapotranspiration.path <- paste0(path0, "/Amazon_new_data/8. Evapotranspiration/03. Working Data")
wind.path <- paste0(path0, "/Amazon_new_data/9. Wind Speed/03. Working Data")
airtemp.path <- paste0(path0, "/Amazon_new_data/10. Air Temperature/03. Working Data")

#---- Load data ----
y_m <- c('2019', '8') 
amaz.var.list <- c(
  paste0(burntArea.path, '/burntarea_working_', y_m[1], '_', y_m[2], '.tif')
  , paste0(landCover.path, '/landcover_working_', y_m[1], '_', y_m[2], '.tif')
  , paste0(precip.path, '/precipitation_working_', y_m[1], '_', y_m[2], '.tif')
  , paste0(soilMoisture.path, '/soilmoisture_working_', y_m[1], '_', y_m[2], '.tif')
  , paste0(elevation.path, '/elevation_working_2019_08.tif')
  , paste0(LandSurfaceTemp.path, '/landsurftemp_working_', y_m[1], '_', y_m[2], '.tif')
  , paste0(humidity.path, '/humidity_working_', y_m[1], '_', y_m[2], '.tif')
  , paste0(evapotranspiration.path, '/evapotranspiration_working_', y_m[1], '_', y_m[2], '.tif')
  , paste0(wind.path, '/wind_working_', y_m[1], '_', y_m[2], '.tif')
  , paste0(airtemp.path, '/airtemp_working_', y_m[1], '_', y_m[2], '.tif')
)
amaz.basin.shp <- st_read("/home/abidm/Documents/Amazon_new_data/0. Amazon_shapefile/projected/amazon_shp_projected.shp")

#---- Prepare data ---- 
tic("Prepare data") 
nbr.z <- 11 # number of zones
# Create 'SpatRaster' object
var.ras <- rast(amaz.var.list) %>% mask(mask = amaz.basin.shp)
names(var.ras) <- c('BurntArea', 'LandCover', 'Precipitation', 'SoilMoisture', 'Elevation', 'LandSurfaceTemp', 'Humidity', 'Evapotranspiration', 'Wind', 'AirTemp')
var.ras[[c('BurntArea', 'SoilMoisture')]][var.ras[[c('BurntArea', 'SoilMoisture')]] < 0] <- NA
# Prepare data table
var0.dt <- as.data.table(var.ras, cell=T, xy=T)
var0.cells.dt <- var0.dt[, c('cell', 'x', 'y')] # to add final prediction
var.dt <- var0.dt %>%
  mutate(
    x = floor(x),
    y = floor(y),
    Year = rep(y_m[1], dim(var0.dt)[1]) %>% as.factor(),
    Month = rep(y_m[2], dim(var0.dt)[1]) %>% as.factor(),
    BurntArea = as.factor(BurntArea),
    LandCover = as.factor(LandCover)
  ) %>%
  na.omit() %>%
  setcolorder(., c('cell', 'x', 'y', 'Year', 'Month'))
var.zones.dt <- splitAmaZones(var.dt)
var.zones.dt$Zones <- as.factor(var.zones.dt$Zones)
toc() # Prepare data: 141.702 sec elapsed

#---- Normalize data & convert to h2o ----
tic("Normalize data") 
AZ_recipe <- recipe( BurntArea ~ ., data = var.zones.dt) %>% 
  step_normalize(all_numeric(), -c(cell, x, y))

var.norm <- AZ_recipe %>% 
  prep() %>% 
  bake(new_data = NULL) %>% 
  setcolorder(., c('Zones', 'cell', 'x', 'y', 'Year', 'Month', 'BurntArea')) %>% 
  as.data.table()
toc() # Normalize data: 12.949 sec elapsed

#---- Model ----
#==============#

tic("Model") 
# Load beta
load(paste0(path0, "/Models_ens/Ens1_err/ENS1_beta.Rdata"))
beta.m <- beta.auc
#
Models <- c('GLM', 'GBM', 'XGB', 'RF')
nbr.m <- length(Models)
p1.lst <- vector(mode='list', length=nbr.z); names(p1.lst) <- paste0('zone', 1:nbr.z)
prob.ens <- data.frame(matrix(ncol = 5, nrow = 0)); colnames(prob.ens) <- c('Zones', 'cell', 'x', 'y', 'p1.ens')
zone <- 1
while(zone <= nbr.z){
  tic(paste0("Zone ", zone))
  zone.name <- paste0('zone', zone)
  var.nz <- var.norm[var.norm$Zones == zone,]
  var.h2o <- as.h2o(var.nz[,-1])
  # Initilization
  nbr.row <- dim(var.nz)[1]
  prob.az = data.frame(matrix(0, nrow = nbr.row, ncol = nbr.m))
  colnames(prob.az) = Models
  #---- Prediction ----
  for (m in Models){ # m <- 'GLM'
    cat("\nZone", zone, "- Model", m)
    # Load model
    cat(": Load model")
    model.az <- h2o.upload_model(paste0(path0, "/Models/", m, "/", m, "_AZ", zone, "_model"))
    # Prediction
    cat(" - Prediction")
    pred.az <- h2o.predict(model.az, newdata=var.h2o)
    # Probability
    cat(" - Probability")
    prob.az[, paste0(m)] <- as.data.frame(pred.az)$p1
    # Clean
    rm(model.az, pred.az)
  }
  
  #---- Compute the probability P(X=1) ----
  p1.meanErr <- Map(`*`, prob.az, beta.meanErr[zone.name, ]) %>% as.data.frame() %>% rowSums()
  p1.auc <- Map(`*`, prob.az, beta.auc[zone.name, ]) %>% as.data.frame() %>% rowSums()
  p1.auc2 <- Map(`*`, prob.az, beta.auc2[zone.name, ]) %>% as.data.frame() %>% rowSums()
  prob.ens <- rbind(prob.ens, data.frame(var.nz[,1:4], p1.meanErr, p1.auc, p1.auc2))
  cat("\n"); toc() 
  # Zone1: 421.631 sec elapsed
  # Zone2: 197.746 sec elapsed
  # Zone3: 284.706 sec elapsed
  # Zone4: 92.767 sec elapsed
  # Zone5: 78.657 sec elapsed
  # Zone6: 80.626 sec elapsed
  # Zone7: 74.978 sec elapsed
  # Zone8: 73.847 sec elapsed
  # Zone9: 163.353 sec elapsed
  # Zone10: 107.491 sec elapsed
  # Zone11: 72.336 sec elapsed
  
  zone <- zone + 1
}
toc() # Model: 1648.896 sec elapsed ~ 27 min
toc() # Total: 1830.1 sec elapsed ~ 31 min

#---- Save ----
save(prob.ens, beta.m, file = paste0(path0, "/Models_ens/Ens1_err/ENS1_pred2_2019_08.Rdata"))

#---- Plot ----
#=============#
amaz.2019.08.ras <- rast(paste0(burntArea.path, '/burntarea_working_', y_m[1], '_', y_m[2], '.tif'))
pred.ras <- amaz.2019.08.ras
values(pred.ras) <- -1
tmp <- as.data.table(pred.ras, cell=T, xy=T)

# Prepare the value of each cell
prob.auc.amaz <- merge(x = as.data.table(prob.ens)[, c('cell', 'p1.auc')], y = tmp[, 'cell'], by = "cell", all = TRUE)

values(pred.ras) <- prob.auc.amaz$p1.auc
writeRaster(pred.ras, paste0(path0, "/Models_ens/Ens1_err/ENS1_pred_2019_08.tif"), overwrite=F)


plot(pred.ras)
amaz.2019.08.ras %>% mask(mask = amaz.basin.shp) %>% plot()

```

```{r}

```

```{r}
var.zones.dt <- splitAmaZones(var.dt)
dim.zones <- var.zones.dt[, .N, by=.(Zones)]
dim.zones[order(Zones)]
```



```{r}
beta.m
rowSums(beta.m)
beta.m ; prob.df
f1.max
```

```{r}
zone <- 1
m <- 'GBM'
```

# Ensemble E2 : H2O.GLM

```{r}
nbr.folds <- 2
for (zone in 1:11){
  tic()
  # zone <- 1
  cat('- zone', zone)
  #---- Load data and models ----
  glm.az <- h2o.upload_model(paste0(path0, "/Models_cv/GLM/GLM_AZ", zone, "_model_", nbr.folds, "f"))
  gbm.az <- h2o.upload_model(paste0(path0, "/Models_cv/GBM/GBM_AZ", zone, "_model_", nbr.folds, "f"))
  # glm.az <- h2o.upload_model(paste0(path0, "/Models/GLM/GLM_AZ", zone, "_model"))
  # gbm.az <- h2o.upload_model(paste0(path0, "/Models/GBM/GBM_AZ", zone, "_model"))
  # xgb.az <- h2o.upload_model(paste0(path0, "/Models/XGB/XGB_AZ", zone, "_model"))
  # rf.az <- h2o.upload_model(paste0(path0, "/Models/RF/RF_AZ", zone, "_model"))
  load(paste0(my.path,"/Amazon_selected_data/data_by_zones_folds/AZ", zone, "_data_train_test_", nbr.folds, "f.Rdata"))
  # convert data to H2O object
  AZ.h2o <- as.h2o(AZ.nz)
  train.h2o <- as.h2o(AZ.trn.folds)
  test.h2o <- as.h2o(AZ.tst)
  # 
  var.names <- colnames(AZ.trn)
  Y <- "BurntArea"
  covt.names <- setdiff(var.names, c("BurntArea", "Zones"))
  
  #---- fit the model ----
  # __ sec elapsed
  tic()
  ens.glm <- h2o.stackedEnsemble(
    model_id = paste0("Ens_GLM_AZ", zone)
    , x = covt.names
    , y = Y
    , training_frame = train.h2o
    , base_models = list(glm.az@model_id, gbm.az@model_id)
    , metalearner_algorithm = "glm"
  )
  toc()

  #---- Save the model ----
  ens.glm.path <- h2o.saveModel(
    object = ens.glm,
    path = paste0(path0, "/Models_ens/GLM"),
    force = FALSE,
    export_cross_validation_predictions = TRUE,
    filename = paste0("GLM_AZ", zone, "_model_ens")
  )

  #---- Load the model ----
  # ens.glm <- h2o.upload_model(paste0(path0, "/Models_ens/GLM/GLM_AZ", zone, "_model"))
  
  #---- Performance ----
  # 1.086 sec elapsed
  ens.glm.perf <- h2o.performance(ens.glm, test.h2o)
  metrics <- c('MSE', 'RMSE', 'LogLoss', 'Mean Per-Class Error', 'AUC', 'AUCPR', 'Gini', 'R^2')
  values <- c(
    glm.az.perf@metrics$MSE,
    glm.az.perf@metrics$RMSE,
    glm.az.perf@metrics$logloss,
    glm.az.perf@metrics$mean_per_class_error,
    glm.az.perf@metrics$AUC,
    glm.az.perf@metrics$pr_auc,
    glm.az.perf@metrics$Gini,
    glm.az.perf@metrics$r2
  )
  perf.df <- data.frame(metrics, values)
  confusion.mtrx <- glm.az.perf@metrics$cm$table
  max.metrics <- glm.az.perf@metrics$max_criteria_and_metric_scores
  save(perf.df, confusion.mtrx, max.metrics, file = paste0(path0, "/Models_ens/GLM/GLM_AZ", zone, "_perf_ens.Rdata"))
  # load(paste0(path0, "/Models_ens/GLM/GLM_AZ", zone, "_perf_ens.Rdata"))

  #---- Variable importance ----
  # Retrieve variable importance
  # h2o.varimp(glm.az)
  
  #---- Plot variable importance ----
  # h2o.varimp_plot(model = glm.az, num_of_features = 13)
  
  #---- Prediction ---- 
  # __ sec elapsed ~ __ min
  tmp <- h2o.predict(ens.glm, newdata=AZ.h2o)
  ens.glm.predic <- h2o:::as.data.frame.H2OFrame(tmp)
  # Save
  save(ens.glm.predic, file = paste0(path0, "/Models_ens/GLM/GLM_AZ", zone, "_predic_ens.Rdata"))
  # load(paste0(path0, "/Models_ens/GLM/GLM_AZ", zone, "_predic_ens.Rdata"))
  
  toc() # 439.142 sec elapsed ~ 8 min

  # Remove variables
  rm(glm.az, glm.az.perf, perf.df, confusion.mtrx, max.metrics, glm.az.predic)
  rm(AZ.nz, AZ.trn, AZ.tst); gc(); gc()
  h2o.removeAll()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
}
```

# Ensemble E3 : H2O.GBM

```{r}
nbr.folds <- 2
for (zone in 1:11){
  tic()
  # zone <- 1
  cat('- zone', zone)
  #---- Load data and models ----
  glm.az <- h2o.upload_model(paste0(path0, "/Models_cv/GLM/GLM_AZ", zone, "_model_", nbr.folds, "f"))
  gbm.az <- h2o.upload_model(paste0(path0, "/Models_cv/GBM/GBM_AZ", zone, "_model_", nbr.folds, "f"))
  # glm.az <- h2o.upload_model(paste0(path0, "/Models/GLM/GLM_AZ", zone, "_model"))
  # gbm.az <- h2o.upload_model(paste0(path0, "/Models/GBM/GBM_AZ", zone, "_model"))
  # xgb.az <- h2o.upload_model(paste0(path0, "/Models/XGB/XGB_AZ", zone, "_model"))
  # rf.az <- h2o.upload_model(paste0(path0, "/Models/RF/RF_AZ", zone, "_model"))
  load(paste0(my.path,"/Amazon_selected_data/data_by_zones_folds/AZ", zone, "_data_train_test_", nbr.folds, "f.Rdata"))
  # convert data to H2O object
  AZ.h2o <- as.h2o(AZ.nz)
  train.h2o <- as.h2o(AZ.trn.folds)
  test.h2o <- as.h2o(AZ.tst)
  # 
  var.names <- colnames(AZ.trn)
  Y <- "BurntArea"
  covt.names <- setdiff(var.names, c("BurntArea", "Zones"))
  
  #---- fit the model ----
  # 24.837 sec elapsed
  tic()
  ens.gbm <- h2o.stackedEnsemble(
    model_id = paste0("Ens_GBM_AZ", zone)
    , x = covt.names
    , y = Y
    , training_frame = train.h2o
    , base_models = list(glm.az@model_id, gbm.az@model_id)
    , metalearner_algorithm = "gbm"
  )
  toc()

  #---- Save the model ----
  ens.gbm.path <- h2o.saveModel(
    object = ens.gbm,
    path = paste0(path0, "/Models_ens/GBM"),
    force = FALSE,
    export_cross_validation_predictions = TRUE,
    filename = paste0("GBM_AZ", zone, "_model_ens")
  )

  #---- Load the model ----
  # ens.gbm <- h2o.upload_model(paste0(path0, "/Models_ens/GBM/GBM_AZ", zone, "_model"))
  
  #---- Performance ----
  # 72.429 sec elapsed
  tic()
  ens.gbm.perf <- h2o.performance(ens.gbm, test.h2o)
  metrics <- c('MSE', 'RMSE', 'LogLoss', 'Mean Per-Class Error', 'AUC', 'AUCPR', 'Gini', 'R^2')
  values <- c(
    ens.gbm.perf@metrics$MSE,
    ens.gbm.perf@metrics$RMSE,
    ens.gbm.perf@metrics$logloss,
    ens.gbm.perf@metrics$mean_per_class_error,
    ens.gbm.perf@metrics$AUC,
    ens.gbm.perf@metrics$pr_auc,
    ens.gbm.perf@metrics$Gini,
    ens.gbm.perf@metrics$r2
  )
  toc()
  perf.df <- data.frame(metrics, values)
  confusion.mtrx <- ens.gbm.perf@metrics$cm$table
  max.metrics <- ens.gbm.perf@metrics$max_criteria_and_metric_scores
  save(perf.df, confusion.mtrx, max.metrics, file = paste0(path0, "/Models_ens/GBM/GBM_AZ", zone, "_perf_ens.Rdata"))
  # load(paste0(path0, "/Models_ens/GBM/GBM_AZ", zone, "_perf_ens.Rdata"))

  #---- Variable importance ----
  # Retrieve variable importance
  # h2o.varimp(ens.gbm)
  
  #---- Plot variable importance ----
  # h2o.varimp_plot(model = ens.gbm, num_of_features = 13)
  
  #---- Prediction ---- 
  # 427.152 sec elapsed ~ __ min
  tic()
  tmp <- h2o.predict(ens.gbm, newdata=AZ.h2o)
  ens.gbm.predic <- h2o:::as.data.frame.H2OFrame(tmp)
  # Save
  save(ens.gbm.predic, file = paste0(path0, "/Models_ens/GBM/GBM_AZ", zone, "_predic_ens.Rdata"))
  # load(paste0(path0, "/Models_ens/GBM/GBM_AZ", zone, "_predic_ens.Rdata"))
  
  toc() # 439.142 sec elapsed ~ 8 min

  # Remove variables
  rm(ens.gbm, ens.gbm.perf, perf.df, confusion.mtrx, max.metrics, ens.gbm.predic)
  rm(AZ.nz, AZ.trn, AZ.tst); gc(); gc()
  h2o.removeAll()
  h2o:::.h2o.garbageCollect(); Sys.sleep(20)
  h2o:::.h2o.garbageCollect(); Sys.sleep(20)
}
```

# Ensemble E4 : H2O.RF

```{r}
nbr.folds <- 2
for (zone in 1:11){
  tic()
  # zone <- 1
  cat('- zone', zone)
  #---- Load data and models ----
  glm.az <- h2o.upload_model(paste0(path0, "/Models_cv/GLM/GLM_AZ", zone, "_model_", nbr.folds, "f"))
  gbm.az <- h2o.upload_model(paste0(path0, "/Models_cv/GBM/GBM_AZ", zone, "_model_", nbr.folds, "f"))
  # glm.az <- h2o.upload_model(paste0(path0, "/Models/GLM/GLM_AZ", zone, "_model"))
  # gbm.az <- h2o.upload_model(paste0(path0, "/Models/GBM/GBM_AZ", zone, "_model"))
  # xgb.az <- h2o.upload_model(paste0(path0, "/Models/XGB/XGB_AZ", zone, "_model"))
  # rf.az <- h2o.upload_model(paste0(path0, "/Models/RF/RF_AZ", zone, "_model"))
  load(paste0(my.path,"/Amazon_selected_data/data_by_zones_folds/AZ", zone, "_data_train_test_", nbr.folds, "f.Rdata"))
  # convert data to H2O object
  AZ.h2o <- as.h2o(AZ.nz)
  train.h2o <- as.h2o(AZ.trn.folds)
  test.h2o <- as.h2o(AZ.tst)
  # 
  var.names <- colnames(AZ.trn)
  Y <- "BurntArea"
  covt.names <- setdiff(var.names, c("BurntArea", "Zones"))
  
  #---- fit the model ----
  # 105.87 sec elapsed
  tic()
  ens.rf <- h2o.stackedEnsemble(
    model_id = paste0("Ens_RF_AZ", zone)
    , x = covt.names
    , y = Y
    , training_frame = train.h2o
    , base_models = list(glm.az@model_id, gbm.az@model_id)
    , metalearner_algorithm = "drf"
  )
  toc()

  #---- Save the model ----
  ens.rf.path <- h2o.saveModel(
    object = ens.rf,
    path = paste0(path0, "/Models_ens/RF"),
    force = FALSE,
    export_cross_validation_predictions = TRUE,
    filename = paste0("RF_AZ", zone, "_model_ens")
  )

  #---- Load the model ----
  # ens.rf <- h2o.upload_model(paste0(path0, "/Models_ens/RF/RF_AZ", zone, "_model"))
  
  #---- Performance ----
  # 80.676 sec elapsed
  tic()
  ens.rf.perf <- h2o.performance(ens.rf, test.h2o)
  metrics <- c('MSE', 'RMSE', 'LogLoss', 'Mean Per-Class Error', 'AUC', 'AUCPR', 'Gini', 'R^2')
  values <- c(
    ens.rf.perf@metrics$MSE,
    ens.rf.perf@metrics$RMSE,
    ens.rf.perf@metrics$logloss,
    ens.rf.perf@metrics$mean_per_class_error,
    ens.rf.perf@metrics$AUC,
    ens.rf.perf@metrics$pr_auc,
    ens.rf.perf@metrics$Gini,
    ens.rf.perf@metrics$r2
  )
  toc()
  perf.df <- data.frame(metrics, values)
  confusion.mtrx <- ens.rf.perf@metrics$cm$table
  max.metrics <- ens.rf.perf@metrics$max_criteria_and_metric_scores
  save(perf.df, confusion.mtrx, max.metrics, file = paste0(path0, "/Models_ens/RF/RF_AZ", zone, "_perf_ens.Rdata"))
  # load(paste0(path0, "/Models_ens/RF/RF_AZ", zone, "_perf_ens.Rdata"))

  #---- Variable importance ----
  # Retrieve variable importance
  # h2o.varimp(ens.rf)
  
  #---- Plot variable importance ----
  # h2o.varimp_plot(model = ens.rf, num_of_features = 13)
  
  #---- Prediction ---- 
  # 475.754 sec elapsed ~ __ min
  tic()
  tmp <- h2o.predict(ens.rf, newdata=AZ.h2o)
  ens.rf.predic <- h2o:::as.data.frame.H2OFrame(tmp)
  # Save
  save(ens.rf.predic, file = paste0(path0, "/Models_ens/RF/RF_AZ", zone, "_predic_ens.Rdata"))
  # load(paste0(path0, "/Models_ens/RF/RF_AZ", zone, "_predic_ens.Rdata"))
  
  toc() # 439.142 sec elapsed ~ 8 min

  # Remove variables
  rm(ens.rf, ens.rf.perf, perf.df, confusion.mtrx, max.metrics, ens.rf.predic)
  rm(AZ.nz, AZ.trn, AZ.tst); gc(); gc()
  h2o.removeAll()
  h2o:::.h2o.garbageCollect(); Sys.sleep(20)
  h2o:::.h2o.garbageCollect(); Sys.sleep(20)
}
```

# Ensemble E5 : H2O.XGB

```{r}
nbr.folds <- 2
for (zone in 1:11){
  tic()
  # zone <- 1
  cat('- zone', zone)
  #---- Load data and models ----
  glm.az <- h2o.upload_model(paste0(path0, "/Models_cv/GLM/GLM_AZ", zone, "_model_", nbr.folds, "f"))
  gbm.az <- h2o.upload_model(paste0(path0, "/Models_cv/GBM/GBM_AZ", zone, "_model_", nbr.folds, "f"))
  # glm.az <- h2o.upload_model(paste0(path0, "/Models/GLM/GLM_AZ", zone, "_model"))
  # gbm.az <- h2o.upload_model(paste0(path0, "/Models/GBM/GBM_AZ", zone, "_model"))
  # xgb.az <- h2o.upload_model(paste0(path0, "/Models/XGB/XGB_AZ", zone, "_model"))
  # rf.az <- h2o.upload_model(paste0(path0, "/Models/RF/RF_AZ", zone, "_model"))
  load(paste0(my.path,"/Amazon_selected_data/data_by_zones_folds/AZ", zone, "_data_train_test_", nbr.folds, "f.Rdata"))
  # convert data to H2O object
  AZ.h2o <- as.h2o(AZ.nz)
  train.h2o <- as.h2o(AZ.trn.folds)
  test.h2o <- as.h2o(AZ.tst)
  # 
  var.names <- colnames(AZ.trn)
  Y <- "BurntArea"
  covt.names <- setdiff(var.names, c("BurntArea", "Zones"))
  
  #---- fit the model ----
  # 16.837 sec elapsed
  tic()
  ens.xgb <- h2o.stackedEnsemble(
    model_id = paste0("Ens_XGB_AZ", zone)
    , x = covt.names
    , y = Y
    , training_frame = train.h2o
    , base_models = list(glm.az@model_id, gbm.az@model_id)
    , metalearner_algorithm = "xgboost"
  )
  toc()

  #---- Save the model ----
  ens.xgb.path <- h2o.saveModel(
    object = ens.xgb,
    path = paste0(path0, "/Models_ens/XGB"),
    force = FALSE,
    export_cross_validation_predictions = TRUE,
    filename = paste0("XGB_AZ", zone, "_model_ens")
  )

  #---- Load the model ----
  # ens.xgb <- h2o.upload_model(paste0(path0, "/Models_ens/XGB/XGB_AZ", zone, "_model"))
  
  #---- Performance ----
  # 72.245 sec elapsed
  tic()
  ens.xgb.perf <- h2o.performance(ens.xgb, test.h2o)
  metrics <- c('MSE', 'RMSE', 'LogLoss', 'Mean Per-Class Error', 'AUC', 'AUCPR', 'Gini', 'R^2')
  values <- c(
    ens.xgb.perf@metrics$MSE,
    ens.xgb.perf@metrics$RMSE,
    ens.xgb.perf@metrics$logloss,
    ens.xgb.perf@metrics$mean_per_class_error,
    ens.xgb.perf@metrics$AUC,
    ens.xgb.perf@metrics$pr_auc,
    ens.xgb.perf@metrics$Gini,
    ens.xgb.perf@metrics$r2
  )
  toc()
  perf.df <- data.frame(metrics, values)
  confusion.mtrx <- ens.xgb.perf@metrics$cm$table
  max.metrics <- ens.xgb.perf@metrics$max_criteria_and_metric_scores
  save(perf.df, confusion.mtrx, max.metrics, file = paste0(path0, "/Models_ens/XGB/XGB_AZ", zone, "_perf_ens.Rdata"))
  # load(paste0(path0, "/Models_ens/XGB/XGB_AZ", zone, "_perf_ens.Rdata"))

  #---- Variable importance ----
  # Retrieve variable importance
  # h2o.varimp(ens.xgb)
  
  #---- Plot variable importance ----
  # h2o.varimp_plot(model = ens.xgb, num_of_features = 13)
  
  #---- Prediction ---- 
  # 412.654 sec elapsed ~ __ min
  tic()
  tmp <- h2o.predict(ens.xgb, newdata=AZ.h2o)
  ens.xgb.predic <- h2o:::as.data.frame.H2OFrame(tmp)
  # Save
  save(ens.xgb.predic, file = paste0(path0, "/Models_ens/XGB/XGB_AZ", zone, "_predic_ens.Rdata"))
  # load(paste0(path0, "/Models_ens/XGB/XGB_AZ", zone, "_predic_ens.Rdata"))
  
  toc() # 

  # Remove variables
  rm(ens.xgb, ens.xgb.perf, perf.df, confusion.mtrx, max.metrics, ens.xgb.predic)
  rm(AZ.nz, AZ.trn, AZ.tst); gc(); gc()
  h2o.removeAll()
  h2o:::.h2o.garbageCollect(); Sys.sleep(20)
  h2o:::.h2o.garbageCollect(); Sys.sleep(20)
}
```

```{r}
nbr.folds <- 2
for (zone in 1:11){
  tic()
  # zone <- 1
  cat('- zone', zone)
  #---- Load data and models ----
  glm.az <- h2o.upload_model(paste0(path0, "/Models_cv/GLM/GLM_AZ", zone, "_model_", nbr.folds, "f"))
  gbm.az <- h2o.upload_model(paste0(path0, "/Models_cv/GBM/GBM_AZ", zone, "_model_", nbr.folds, "f"))
  # xgb.az <- h2o.upload_model(paste0(path0, "/Models/XGB/XGB_AZ", zone, "_model"))
  # rf.az <- h2o.upload_model(paste0(path0, "/Models/RF/RF_AZ", zone, "_model"))
  load(paste0(my.path,"/Amazon_selected_data/data_by_zones/AZ", zone, "_data_train_test.Rdata"))
  # convert data to H2O object
  AZ.h2o <- as.h2o(AZ.nz)
  train.h2o <- as.h2o(AZ.trn)
  test.h2o <- as.h2o(AZ.tst)
  # 
  var.names <- colnames(AZ.trn)
  Y <- "BurntArea"
  covt.names <- setdiff(var.names, c("BurntArea", "Zones"))
  
  #---- fit the model ----
  # __ sec elapsed
  tic()
  ens.glm <- h2o.stackedEnsemble(
    model_id = paste0("Ens_GLM_AZ", zone)
    , x = covt.names
    , y = Y
    , training_frame = train.h2o
    , base_models = list(glm.az@model_id, gbm.az@model_id)
    , metalearner_algorithm = "glm"
  )
  toc()

  #---- Save the model ----
  ens.glm.path <- h2o.saveModel(
    object = ens.glm,
    path = paste0(path0, "/Models_ens/GLM"),
    force = FALSE,
    export_cross_validation_predictions = TRUE,
    filename = paste0("GLM_AZ", zone, "_model_ens")
  )

  #---- Load the model ----
  # ens.glm <- h2o.upload_model(paste0(path0, "/Models_ens/GLM/GLM_AZ", zone, "_model"))
  
  #---- Performance ----
  # 1.086 sec elapsed
  ens.glm.perf <- h2o.performance(ens.glm, test.h2o)
  metrics <- c('MSE', 'RMSE', 'LogLoss', 'Mean Per-Class Error', 'AUC', 'AUCPR', 'Gini', 'R^2')
  values <- c(
    glm.az.perf@metrics$MSE,
    glm.az.perf@metrics$RMSE,
    glm.az.perf@metrics$logloss,
    glm.az.perf@metrics$mean_per_class_error,
    glm.az.perf@metrics$AUC,
    glm.az.perf@metrics$pr_auc,
    glm.az.perf@metrics$Gini,
    glm.az.perf@metrics$r2
  )
  perf.df <- data.frame(metrics, values)
  confusion.mtrx <- glm.az.perf@metrics$cm$table
  max.metrics <- glm.az.perf@metrics$max_criteria_and_metric_scores
  save(perf.df, confusion.mtrx, max.metrics, file = paste0(path0, "/Models_ens/GLM/GLM_AZ", zone, "_perf_ens.Rdata"))
  # load(paste0(path0, "/Models_ens/GLM/GLM_AZ", zone, "_perf_ens.Rdata"))

  #---- Variable importance ----
  # Retrieve variable importance
  # h2o.varimp(glm.az)
  
  #---- Plot variable importance ----
  # h2o.varimp_plot(model = glm.az, num_of_features = 13)
  
  #---- Prediction ---- 
  # __ sec elapsed ~ __ min
  tmp <- h2o.predict(ens.glm, newdata=AZ.h2o)
  ens.glm.predic <- h2o:::as.data.frame.H2OFrame(tmp)
  # Save
  save(ens.glm.predic, file = paste0(path0, "/Models_ens/GLM/GLM_AZ", zone, "_predic_ens.Rdata"))
  # load(paste0(path0, "/Models_ens/GLM/GLM_AZ", zone, "_predic_ens.Rdata"))
  
  toc() # 439.142 sec elapsed ~ 8 min

  # Remove variables
  rm(glm.az, glm.az.perf, perf.df, confusion.mtrx, max.metrics, glm.az.predic)
  rm(AZ.nz, AZ.trn, AZ.tst); gc(); gc()
  h2o.removeAll()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
  Sys.sleep(10); h2o:::.h2o.garbageCollect()
}
```

# Plot AUC

```{r}
# Load data from models
Models <- c(rep('GLM', 11), rep('GBM', 11), rep('XGB', 11), rep('RF', 11))
nbr.models <- length(Models)/11
Zones <- rep(c(1:11), nbr.models) %>% as.factor()
AUC <- rep(0, 11*nbr.models)
AUC.df <- data.frame(Zones, Models, AUC)
for (m in Models){
  for (zone in 1:11) {
    load(file = paste0(path0, "/Models/", m, "/", m, "_AZ", zone, "_perf.Rdata"))
    AUC.df[(Zones==zone) & (Models==m), 'AUC'] <- perf.df[perf.df$metrics=='AUC',]$values
  }
}
# Load data Ensembles
# load(paste0(path0, "/Models_ens/Ens1_err/ENS1_perf_pred.Rdata"))
Ens1.df <- data.frame(as.factor(1:11), rep('Ens1', 11), as.vector(auc.m))
colnames(Ens1.df) <- c('Zones', 'Models', 'AUC')
# Concatenate data
AUC.df <- rbind(AUC.df, Ens1.df)
AUC.dt <- dcast(as.data.table(AUC.df), Zones ~ Models, values.var = c('AUC')) %>% setcolorder(., c('Zones', 'Ens1', 'RF', 'XGB', 'GBM', 'GLM'))

print(AUC.dt)
# Plot
plot_ly(
  AUC.df
  , x = ~Zones
  , y = ~Models
  , z = ~AUC
  , intensity = ~AUC, type = 'mesh3d'
  # ,  colors = colorRamp(c("blue", "lightblue", "chartreuse3", "yellow", "red"))
  ,  colors = colorRamps::matlab.like(4000)
  # ,  colors = colorRamps::blue2red(4)
  # ,  colors = grDevices::rainbow(4)
)
```

# Plot the probability

```{r}
p1.lst
p1.ens1 <- merge(p1.lst)
length(p1.ens1)
```









# Clear and Shutdown H2O

```{r}
h2o.removeAll()
h2o:::.h2o.garbageCollect()
h2o:::.h2o.garbageCollect()
h2o:::.h2o.garbageCollect()
h2o.ls()
# h2o.shutdown(prompt = FALSE)


# Installation
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }
if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }
# 
# pkgs <- c("RCurl","jsonlite")
# for (pkg in pkgs) {
#   if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }
# }
# 
# install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))

require(devtools)
install_version("h2o", version = "3.38.0.1", repos = "http://cran.us.r-project.org", INSTALL_opts = '--no-lock')

library(h2o)

```

































































































